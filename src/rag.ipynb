{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community langchain-openai langchainhub chromadb tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nbstripout -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-text-splitters -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Warm up & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "import pandas as pd \n",
    "from operator import itemgetter\n",
    "\n",
    "import langchain\n",
    "print(\"langchain.__version__ \", langchain.__version__)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.load import loads, dumps\n",
    "\n",
    "# for lcel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_base_dir = '../data/processed/p_jsons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the len of each doc\n",
    "all_len = []\n",
    "all_char_len = []\n",
    "for item in os.listdir(raw_docs_base_dir):\n",
    "    with open(os.path.join(raw_docs_base_dir, item), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_len.append(len(' '.join(data['doc_judgement']).split()))\n",
    "        all_char_len.append(len(' '.join(data['doc_judgement'])))\n",
    "\n",
    "pprint(pd.Series(all_len).describe())\n",
    "pprint(pd.Series(all_char_len).describe())\n",
    "\n",
    "print('Percent len > 7k: ', (len([item for item in all_len if item > 7000])/len(all_len)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_chunker(text: str):\n",
    "\n",
    "    # based on len of doc, we can set different chunk size\n",
    "    num_chars = len(text)\n",
    "\n",
    "    if num_chars < 3000:\n",
    "        return [text]\n",
    "    \n",
    "    elif num_chars > 3000 and num_chars < 12000:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=3000, \n",
    "                                                  chunk_overlap=300, \n",
    "                                                  separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        return splitter.split_text(text)\n",
    "        \n",
    "    else:\n",
    "        coarse_splitter = RecursiveCharacterTextSplitter(chunk_size=9000,\n",
    "                                                         chunk_overlap=900,\n",
    "                                                         separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        coarse_chunks = coarse_splitter.split_text(text)\n",
    "        fine_splitter = RecursiveCharacterTextSplitter(chunk_size=3000,\n",
    "                                                       chunk_overlap=300,\n",
    "                                                       separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        final_chunks = []\n",
    "        for coarse_chunk in coarse_chunks:\n",
    "            fine_chunks = fine_splitter.split_text(coarse_chunk)\n",
    "            final_chunks.extend(fine_chunks)\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exec: All chunks Extraction \n",
    "\n",
    "chunks_all = []\n",
    "for item in os.listdir(raw_docs_base_dir):\n",
    "    with open(os.path.join(raw_docs_base_dir, item), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        doc_text = ' '.join(data['doc_judgement'])\n",
    "        chunks = custom_chunker(doc_text)\n",
    "        print(f\"Document: {item}, Original Length: {len(doc_text)}, Number of Chunks: {len(chunks)}\")\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_metadata = {\n",
    "                'source_doc': item,\n",
    "                'chunk_index': idx,\n",
    "                'chunk_length': len(chunk),\n",
    "                'original_length': len(doc_text)\n",
    "            }\n",
    "            chunks_all.append((chunk, chunk_metadata))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "any([item[1]['chunk_length'] > 3000 for item in chunks_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_documents = [Document(page_content=item[0], metadata=item[1]) for item in chunks_all]\n",
    "print(len(lc_documents))\n",
    "\n",
    "import random\n",
    "print(random.choice(lc_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediatory dump of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the chroma dir\n",
    "\n",
    "tmp_db_dir = '/tmp/chroma_db_test_vf'\n",
    "\n",
    "## enable only if you want to clear existing db otherwise it can be used to load existing one\n",
    "# if os.path.exists(tmp_db_dir):\n",
    "#     print(f\"Removing existing db dir: {tmp_db_dir}\")\n",
    "#     shutil.rmtree(tmp_db_dir)\n",
    "\n",
    "vector_store_chroma = Chroma(collection_name='legal_mini_rag', \n",
    "                             embedding_function=OpenAIEmbeddings(),\n",
    "                             persist_directory=tmp_db_dir,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total documents:\", vector_store_chroma._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_chroma.add_documents(lc_documents)\n",
    "# vector_store_chroma.persist() # to save them to disk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test collection \n",
    "my_collection = vector_store_chroma._collection\n",
    "print('Total docs indexed: ', my_collection.count())\n",
    "\n",
    "random_embedding = my_collection.get(include=[\"embeddings\"], limit=1)\n",
    "print('embedding len: ', random_embedding['embeddings'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_ = vector_store_chroma.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ = \"real estate case outcomes and judgments?\"\n",
    "\n",
    "fetched_docs = retriever_.invoke(query_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(fetched_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ = ls_client.pull_prompt(\"rlm/rag-prompt\")\n",
    "print(prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaining it together using LCEL\n",
    "\n",
    "def format_content(docs):\n",
    "    return '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "# rag_chain\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever_ | format_content, \"question\": RunnablePassthrough()}\n",
    "    | prompt_\n",
    "    | llm_\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asking a question using the RAG chain\n",
    "\n",
    "response = rag_chain.invoke(\"whcih legislations are being used in real estate cases?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Optimizing RAG Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Advance Query Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Multi Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for generating multiple queries\n",
    "multi_query_template_ = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "multi_query_prompt_ = ChatPromptTemplate.from_template(multi_query_template_)\n",
    "\n",
    "generate_new_queries_chain = (\n",
    "    multi_query_prompt_\n",
    "    | llm_\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate multiple queries for a sample question\n",
    "\n",
    "tmp_question = 'which legislations are being used in real estate cases?'\n",
    "\n",
    "generated_queries = generate_new_queries_chain.invoke({'question': tmp_question})\n",
    "\n",
    "pprint(generated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrating it back into main Retrieval chain\n",
    "\n",
    "def process_output_of_multi_query(doc_list):\n",
    "\n",
    "    flattened_docs = [dumps(doc) for sublist in doc_list for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "rag_chain_multi_query = (\n",
    "    generate_new_queries_chain |\n",
    "    retriever_.map() |\n",
    "    process_output_of_multi_query)\n",
    "\n",
    "multi_query_context = rag_chain_multi_query.invoke({'question': tmp_question})\n",
    "\n",
    "print(\"Docs fetched using multi query RAG: \", len(multi_query_context))\n",
    "pprint(multi_query_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## integrating multi query context into Generation chain\n",
    "\n",
    "mq_rag_chain = (\n",
    "\n",
    "    {\"context\": rag_chain_multi_query, \"question\": itemgetter('question')}\n",
    "    | prompt_\n",
    "    | llm_\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(mq_rag_chain.invoke({'question': tmp_question}))\n",
    "\n",
    "\n",
    "## conclusion on experiment: better in terms of variety with multi query RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### RAG Fusion (on top of Multi Query)\n",
    "\n",
    "Re-ranking docs using technique: RRF (Reciprocal Rank Fusion)\n",
    "\n",
    "RRF is a simple scoring method to merge ranked search results from multiple retrievers by giving higher weight to top-ranked items using reciprocal rank scoring.\n",
    "\n",
    "RRF_score = 1 / (k + rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ip = [[a,b,c],[d,b,f],[g,d,a]]\n",
    "\n",
    "def get_rrf_docs(docs, k = 60):\n",
    "\n",
    "    fusion_score_dict = {}\n",
    "\n",
    "    for doc_set in docs:\n",
    "        for rank, doc in enumerate(doc_set):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fusion_score_dict:\n",
    "                fusion_score_dict[doc_str] = 0\n",
    "            fusion_score_dict[doc_str] += 1 / (k + rank)\n",
    "    \n",
    "    reranked_docs = [(loads(doc_str), score) for doc_str, score in sorted(fusion_score_dict.items(), key=itemgetter(1), reverse=True)]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## integrate it back into main Retrieval chain\n",
    "\n",
    "template_rrf = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output:\"\"\"\n",
    "\n",
    "rrf_prmpt_ = ChatPromptTemplate.from_template(template_rrf)\n",
    "\n",
    "multi_queries_rrf_chain = (\n",
    "    rrf_prmpt_ \n",
    "    | llm_ \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split('\\n'))\n",
    ")\n",
    "\n",
    "retrieval_chain_rrf = (\n",
    "    multi_queries_rrf_chain |\n",
    "    retriever_.map() |\n",
    "    get_rrf_docs\n",
    ")\n",
    "\n",
    "docs =  retrieval_chain_rrf.invoke({'question': tmp_question})\n",
    "\n",
    "print(\"Docs fetched using multi query RAG: \", len(docs))\n",
    "print(\"RRF Retrieval: \", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## integrate into generation chain\n",
    "\n",
    "generation_chain = (\n",
    "\n",
    "    {\"context\": retrieval_chain_rrf, \"question\": itemgetter('question')}\n",
    "    | prompt_\n",
    "    | llm_\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(mq_rag_chain.invoke({'question': tmp_question}))\n",
    "\n",
    "\n",
    "## conclusion on experiment: #num of acts (entites) caputured better than with RRF based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
