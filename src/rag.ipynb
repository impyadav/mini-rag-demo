{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain-openai langchainhub chromadb tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nbstripout -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-text-splitters -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import langchain\n",
    "print(\"langchain.__version__ \", langchain.__version__)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Warm up & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_base_dir = '../data/processed/p_jsons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the len of each doc\n",
    "all_len = []\n",
    "all_char_len = []\n",
    "for item in os.listdir(raw_docs_base_dir):\n",
    "    with open(os.path.join(raw_docs_base_dir, item), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_len.append(len(' '.join(data['doc_judgement']).split()))\n",
    "        all_char_len.append(len(' '.join(data['doc_judgement'])))\n",
    "\n",
    "pprint(pd.Series(all_len).describe())\n",
    "pprint(pd.Series(all_char_len).describe())\n",
    "\n",
    "print('Percent len > 7k: ', (len([item for item in all_len if item > 7000])/len(all_len)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_chunker(text: str):\n",
    "\n",
    "    # based on len of doc, we can set different chunk size\n",
    "    num_chars = len(text)\n",
    "\n",
    "    if num_chars < 3000:\n",
    "        return [text]\n",
    "    \n",
    "    elif num_chars > 3000 and num_chars < 12000:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=3000, \n",
    "                                                  chunk_overlap=300, \n",
    "                                                  separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        return splitter.split_text(text)\n",
    "        \n",
    "    else:\n",
    "        coarse_splitter = RecursiveCharacterTextSplitter(chunk_size=9000,\n",
    "                                                         chunk_overlap=900,\n",
    "                                                         separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        coarse_chunks = coarse_splitter.split_text(text)\n",
    "        fine_splitter = RecursiveCharacterTextSplitter(chunk_size=3000,\n",
    "                                                       chunk_overlap=300,\n",
    "                                                       separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "        final_chunks = []\n",
    "        for coarse_chunk in coarse_chunks:\n",
    "            fine_chunks = fine_splitter.split_text(coarse_chunk)\n",
    "            final_chunks.extend(fine_chunks)\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exec: All chunks Extraction \n",
    "\n",
    "chunks_all = []\n",
    "for item in os.listdir(raw_docs_base_dir):\n",
    "    with open(os.path.join(raw_docs_base_dir, item), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        doc_text = ' '.join(data['doc_judgement'])\n",
    "        chunks = custom_chunker(doc_text)\n",
    "        print(f\"Document: {item}, Original Length: {len(doc_text)}, Number of Chunks: {len(chunks)}\")\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_metadata = {\n",
    "                'source_doc': item,\n",
    "                'chunk_index': idx,\n",
    "                'original_length': len(doc_text)\n",
    "            }\n",
    "            chunks_all.append((chunk, chunk_metadata))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_documents = [Document(page_content=item[0], metadata=item[1]) for item in chunks_all]\n",
    "print(len(lc_documents))\n",
    "\n",
    "import random\n",
    "print(random.choice(lc_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the chroma dir\n",
    "vector_store_chroma = Chroma(collection_name='legal_mini_rag', \n",
    "                             embedding_function=OpenAIEmbeddings(),\n",
    "                             persist_directory='/tmp/chroma_db_test'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_chroma.add_documents(lc_documents)\n",
    "# vector_store_chroma.persist() # to save them to disk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test collection \n",
    "my_collection = vector_store_chroma._collection\n",
    "print('Total docs indexed: ', my_collection.count())\n",
    "\n",
    "random_embedding = my_collection.get(include=[\"embeddings\"], limit=1)\n",
    "print('embedding len: ', random_embedding['embeddings'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
